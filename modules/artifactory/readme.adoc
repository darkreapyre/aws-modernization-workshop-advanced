= Digital Modernization

:imagesdir: ../../images
:icons: font

== Artifactory
////
NOTE: *This is an optional lab for an introduction to leveraging better security principles by using a binary repository manager. Click to the next module should you wish to skip this lab.*
////

****
*Expected Outcome:*

* 100 level understanding of a binary repository manager.
* Deploy JFrog Artifactory for Continuous Updates.

*Lab Requirements:*
Cloud9 IDE

*Average Lab Time:*
45-60 minutes
****

=== Introduction

In this module, we will deploy the OpenSource version of link:https://jfrog.com/artifactory/[JFrog Artifactory] using the managed ECS Fargate service, to your AWS account using a CloudFormation template.

JFrog Artifactory is an advanced *Binary Repository Manager* for use by build tools (like Maven and Gradle), dependency management tools (like Ivy and NuGet) and build servers (like Jenkins, Hudson, TeamCity and Bamboo). Repository managers serve two purposes: 

. They act as highly configurable proxies between your organization and external repositories.
. Provide build servers with a deployment destination for your internally generated artifacts.

==== Getting Started with JFrog Artifactory

Before we get started in deploying Artifactory, lets first change to the module directory.

[source,shell]
----
cd ~/environment/aws-modernization-workshop/modules/artifactory
----

To deploy JFrog Artifactory we'll be using CloudFormation.

****
CloudFormation creates the following resources:

* `VPC` - Virtual Private CLoud for the Fargate Cluster.
* `ECS Cluster` - Cluster to manage Fargate tasks.
* `ECS Task` - Artifactory OSS Fargate containers.
****

The steps to Deploy Artifactory using a CloudFormation template are:

. Launch the CloudFormation Teamplte.
. Login and configure Artifactory.
. Configure code repositories.
. Modify and build containers.

===== 1a. Launch the Cloudformation template

To launch the CloudFormation `CREATE` process, run the following command in the Cloud9 `terminal`.

[source,shell]
----
aws cloudformation create-stack --stack-name "artifactory-oss" \
  --template-body=file://artifactory-oss.yml \
  --parameters ParameterKey=ClusterName,ParameterValue="artifactory-oss" \
  --capabilities CAPABILITY_IAM
----

Sample output:
[.output]
....
{
    "StackId": "arn:aws:cloudformation:us-west-2:<REDACTED>:stack/artifactory-oss/b6822bf0-767b-11e9-8e3a-0a95c68a7df8"
}
....

To wait for the CloudFormation template to successfully deploy, run the following:

[source,shell]
----
until [[ `aws cloudformation describe-stacks \
  --stack-name "artifactory-oss" \
  --query "Stacks[0].[StackStatus]" \
  --output text` == "CREATE_COMPLETE" ]]; \
  do  echo "The stack is NOT in a state of CREATE_COMPLETE at `date`"; \
  sleep 30; done && echo "The Stack is built at `date` - Please proceed"
----

TIP: You can also view the status of the CloudFormation `CREATE` process by openning the link:https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2[Cloudformation Console] in a browser tab and viewing the stack.

===== 1b. Get the container IP
In a production environment, it is a good practice to front-end your ECS or Fargate services using an link:https://aws.amazon.com/elasticloadbalancing/[Amazon Application LoadBalancer]. Howeever, for the purpose of this lab, we will simply connect directly to the container public IP address. Run the following command to output the public IP address:

[source,shell]
----
aws ec2 describe-network-interfaces \
  --network-interface-ids=$(aws ecs describe-tasks --cluster=artifactory-oss \
    --tasks=`aws ecs list-tasks \
    --cluster=artifactory-oss \
    --query taskArns[0] \
    --output=text` \
    --query tasks[0].attachments[0].details[1].value \
    --output=text) \
  --query NetworkInterfaces[0].Association.PublicIp \
  --output=text
----

==== 2a. Log into Artifactory and set up a new admin password.
Open up your browser to \`http://<ip-from-previous-step>:8081`. For example: \http://54.202.45.157:8081. You should be presented with a window that looks similar to the one below.


==== 2b. Configuring JFrog Artifactory

Steps to Configure Artifactory for the first time:

Step 1:: You should see a *Welcome to JFrog Artifactory!* welcome screen like below. Click *Next*.

image::artifactory-01.PNG[artifactory]

Step 2:: Provide a super secure password for your instance of Artifactory. Click *Create*.

image::artifactory-02.PNG[artifactory]

Step 3:: We will *_NOT_* configure a proxy server for our lab environment. Click *Skip*.

image::artifactory-03.PNG[artifactory]

Step 4:: Now, we will create our first repository. Select *Maven* from the list. Click *Create*.

image::artifactory-04.PNG[artifactory]

Step 5:: You have successfully configured Artifactory and should see a message like this. Click *Finish*.

image::artifactory-05.PNG[artifactory]

NOTE: Additional information can be found in the link:https://www.jfrog.com/confluence/display/RTF/Welcome+to+Artifactory[JFrog Artifactory User Guide].

==== 3. Configure the Maven repositories.
For the purpose of this lab, we will only be simulating the process of preemptively reviewing libraries as discussed in our *Security* discussion. So in order for our build process to succeed, we also need to add some upstream repositories to Artifactory.

Step 1:: Open the Admin Interface by clicking *Admin* on the left and click on *Remote* under the *Repositories* header.

image::artifactory-12.png[artifactory]

Step 2:: Click on the *+ New* button in the top right corner, and select *Maven* from the Package Type dialog that opens. In the new form that opens, fill in the fields as shown in the image below: 

image::artifactory-13.JPG[artifactory]

Then click *Test*. When the test succeeds, click *Save & Finish* in the bottom right.

Step 3:: We now need to edit the virtual repository, to include the newly added remote repository. On the left side menu, open the *Admin* panel again and select *Virtual* under the repositories section. Select the *libs-release* repository. A new window like the one below should open.

image::artifactory-14.JPG[artifactory]

Step 4:: Move the *primefaces* repository from the *available repositories list* to the *Selected repositories list* by clicking on *primefaces* and then using the green *>* button. Now click *Save and Finish*

==== 4. Modify our Maven Container Build
Now that we have our Artifactory repositories correctly configured, we need to modify the maven settings for our application and have it pull the libraries from the secured repo. We do this by editing the `settings.xml` file for maven.

We have a pre-written `settings.xml` for you, but we need to replace some of the info inside it, with info specific to your deployment.

Step 1:: We need to get the public IP from the artifactory container again. This time, we will also store it as an Environment Variable. Using the Cloud9 `terminal`, Run the following command to create a variable called `ART_IP`.

[source,shell]
----
ART_IP=$(aws ec2 describe-network-interfaces \
  --network-interface-ids=$(aws ecs describe-tasks \
  --cluster=artifactory-oss --tasks=`aws ecs list-tasks \
  --cluster=artifactory-oss --query taskArns[0] --output=text` \
  --query tasks[0].attachments[0].details[1].value --output=text) \
  --query NetworkInterfaces[0].Association.PublicIp --output=text)
----

Step 2:: Add the IP to our `settings.xml` by running the following command:
[source,shell]
----
sed -i "s/<artifact-ip>/$ART_IP/" settings.xml
----

Step 3:: Make some modifications to `Dockerfile`
Now that we have the repository information saved in the `settings.xml` for maven, we also need to make sure that Docker copies the file into the new build environment. We _could_ accomplish that by simply adding a single line to the existing `Dockerfile`, as shown below.

[source,shell]
----
COPY ./settings.xml /root/.m2/
----

However, to save some time, we have already done this for you on line `#8` of the `Dockerfile` in the current working directory. We just need you to copy the `settings.xml` and `Dockerfile` into the container app directory by running the following commands.

[source,shell]
----
cp {settings.xml,Dockerfile} \
~/environment/aws-modernization-workshop/modules/containerize-application/
----

Your `Dockerfile` in the `~/environment/aws-modernization-workshop/modules/containerize-application/` direcrtory, should look as follows:

[source,shell]
----
FROM maven:3.5-jdk-7 AS build

# set the working directory
WORKDIR /usr/src/app

# copy the POM and Maven Settings
COPY ./app/pom.xml /usr/src/app/pom.xml
COPY ./settings.xml /root/.m2/

# just install the dependencies for caching
RUN mvn dependency:go-offline

# copy the application code
COPY ./app /usr/src/app

# package the application
RUN mvn package -Dmaven.test.skip=true

# create our Wildfly based application server
FROM jboss/wildfly:11.0.0.Final AS application

# install postgresql support
RUN mkdir -p $JBOSS_HOME/modules/system/layers/base/org/postgresql/main
COPY ./postgresql $JBOSS_HOME/modules/system/layers/base/org/postgresql/main
RUN /bin/sh -c '$JBOSS_HOME/bin/standalone.sh &' \
  && sleep 10 \
  && $JBOSS_HOME/bin/jboss-cli.sh --connect --command="/subsystem=datasources/jdbc-driver=postgresql:add(driver-name=postgresql,driver-module-name=org.postgresql, driver-class-name=org.postgresql.Driver)" \
  && $JBOSS_HOME/bin/jboss-cli.sh --connect --command=:shutdown \
  && rm -rf $JBOSS_HOME/standalone/configuration/standalone_xml_history/ \
  && rm -rf $JBOSS_HOME/standalone/log/*

# copy and deploy the war file from build layer to application layer
COPY --from=build /usr/src/app/target/applicationPetstore.war /opt/jboss/wildfly/standalone/deployments/applicationPetstore.war

# copy our configuration
COPY ./standalone.xml /opt/jboss/wildfly/standalone/configuration/standalone.xml

# install nc for entrypoint script and copy the entrypoint script
USER root
RUN yum install nc -y
USER jboss
COPY ./docker-entrypoint.sh /opt/jboss/docker-entrypoint.sh

# expose the application port and the management port
EXPOSE 8080 9990

# run the application
ENTRYPOINT [ "/opt/jboss/docker-entrypoint.sh" ]
CMD [ "-b", "0.0.0.0", "-bmanagement", "0.0.0.0" ]
----

Step 4::
Now that we have reconfigured our Docker containers we need to rebuild these images. First we need to go back to the `containerized-application` directory.

[source,shell]
----
cd ~/environment/aws-modernization-workshop/modules/containerize-application
----

Now that we are back in the *Containerize Application* folder we can rerun `docker-compose build` command.

[source,shell]
----
docker-compose build petstore
----

Once the container has been rebuilt using the Artifactory repositories, we are ready to move on to the next module.